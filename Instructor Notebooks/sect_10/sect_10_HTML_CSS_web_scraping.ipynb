{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDKmrDSfq6L7"
   },
   "source": [
    "# Section 10: HTML, CSS, & WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDKmrDSfq6L7"
   },
   "source": [
    "> Web Scraping 101 & HTML/CSS beyond web-scraping\n",
    "\n",
    "- 05/22/20\n",
    "- onl01-dtsc-pt-041320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JU_U8mWD-WLR"
   },
   "source": [
    "## Learning Objectives / Outline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Wnkf6zBwjsF"
   },
   "source": [
    "\n",
    "- **Part 1: HTML & CSS: Beyond Web Scraping**\n",
    "    - Brief Overview of HTML & CSS\n",
    "    - Learn when you will use HTML & CSS in your data science journey\n",
    "    - Demonstrate the power of CSS with a Plotly/Dash dashboard. \n",
    "    - Demonstrate the value of learning HTML/CSS with VS Code.\n",
    "    <br><br>\n",
    "\n",
    "- **Part 2: Walk through the basics of web scraping:**\n",
    "    - Learn to use Chrome's Inspect tool to hunt down target website data\n",
    "    - Learn how to use Beautiful Soup to scrape the contents of a web page. \n",
    "\n",
    "- **Part 3: Learn.co Web Scraping Lab**\n",
    "- ~~**Create our own web scraping functions  to add to `fsds_100719`?**~~\n",
    "\n",
    " \n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSGt5VhvJ_3c"
   },
   "source": [
    "# ðŸ““ Part 1: HTML & CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ahn6EqsGR53"
   },
   "source": [
    "- HMTL is responsible for the _content_ of a website.\n",
    "- CSS is responsible for the appearance / layout of a website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSwUhVVTwvRq"
   },
   "source": [
    "## HTML Overview & Tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jZuk4QQ17nU"
   },
   "source": [
    "- All HTML pages have the following components\n",
    "    1. document declaration followed by html tag\n",
    "    \n",
    "    `<!DOCTYPE html>`<br>\n",
    "    `<html>`\n",
    "    2. Head\n",
    "     html tag<br>\n",
    "    `<head> <title></title></head>`\n",
    "    3. Body<br>\n",
    "    `<body>` ... content... `</body>`<br>\n",
    "    `</html>`\n",
    "\n",
    "\n",
    "\n",
    "- Html content is divdied into **tags** that specify the type of content.\n",
    "    - [Basic Tags Reference Table](https://www.w3schools.com/tags/ref_byfunc.asp)\n",
    "    - [Full Alphabetical Tag Reference Table](https://www.w3schools.com/tags/)\n",
    "    \n",
    "    - **tags** have attributes\n",
    "        - [Tag Attributes](https://www.w3schools.com/html/html_attributes.asp)\n",
    "        - Attributes are always defined in the start/opening tag. \n",
    "\n",
    "    - **tags** may have several content-creator-defined attributes such as `class` or `id`\n",
    "    \n",
    "    \n",
    "- We will **use the tag and its identifying attributes to isolate content** we want on a web page with BeautifulSoup.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-hbk2aTEzLw"
   },
   "source": [
    "## CSS Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hvax5OIHGsI"
   },
   "source": [
    "\n",
    "### List the Components of CSS\n",
    "*Excerpt From Section 13: Intro to CSS*\n",
    "\n",
    ">For each **presentation rule**, there are 3 things to keep in mind:\n",
    "1. What is the specific HTML we want to style?\n",
    "2. What are the qualities we want to modify (e.g. the properties of text\n",
    "   in a paragraph)?\n",
    "3. _How_ do we want to modify the qualities of the element (e.g. font\n",
    "   family, font color, font size, line height, letter spacing, etc.)?\n",
    "\n",
    "\n",
    "> CSS **selectors** are a way of declaring which HTML elements you wish to style.\n",
    "Selectors can appear a few different ways:\n",
    "- The type of HTML element(`h1`, `p`, `div`, etc.)\n",
    "- The value of an element's `id` or `class` (`<p id='idvalue'></p>`, `<p\n",
    "  class='classname'></p>`)\n",
    "- The value of an element's attributes (`value=\"hello\"`)\n",
    "- The element's relationship with surrounding elements (a `p` within an element\n",
    "  with class of `.infobox`)\n",
    "\n",
    "[Type selectors documentation](https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors)\n",
    "\n",
    "The element type `class` is a commonly used selector. Class selectors are used\n",
    "to **select all elements that share a given class name**. The class selector\n",
    "syntax is: `.classname`. Prefix the class name with a '.'(period).\n",
    "\n",
    "```css\n",
    "/*\n",
    "select all elements that have the 'important-topic' classname (e.g. <h1 class='important-topic'>\n",
    "and <h1 class='important-topic'>)\n",
    "*/\n",
    ".important-topic\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0JWx49oH6fb"
   },
   "source": [
    "\n",
    "You can also use the `id` selector to style elements. However, **there should\n",
    "be only one element with a given id** in an HTML document. This can make\n",
    "styling with the ID selector ideal for one-off styles. The `id` selector syntax\n",
    "is: `#idvalue`. Prefix the id attribute of an element with a `#` (which is\n",
    "called \"octothorpe,\" \"pound sign\", or \"hashtag\").\n",
    "\n",
    "```css\n",
    "/*\n",
    "selects the HTML element with the id 'main-header' (e.g. <h1 id='main-header'>)\n",
    "*/\n",
    "#main-header\n",
    "\n",
    "```\n",
    "\n",
    "[id selectors documentation](https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DXm5OZKpaZg"
   },
   "source": [
    "## When will/can I use HTML & CSS?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT9Mk2TaEpNt"
   },
   "source": [
    "1. Web scraping.\n",
    "\n",
    "2. Adding Images and links to your Markdown Documents/Blog Posts\n",
    "- Inside your notebook with Markdown HTML tags for alignment, images, etc.\n",
    "\n",
    "3. Controlling the appearance of Pandas with CSS\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html\n",
    "    \n",
    "    \n",
    "4. ipywidget layouts (i.e. fsds_100719.ihelp_menu)\n",
    "\n",
    "\n",
    "5. <details>\n",
    "<summary style=\"font-weight:bold\">Your Notebooks</summary>\n",
    "<div style=\"color:blue;display:block;text-align:center;border: solid purple 2px;font-family:serif;font-size:3rem;padding:2rem;background-color:lightgreen;width:50%;padding:2em\"><br>YOUR NOTEBOOKS!</div>\n",
    "</details>\n",
    "\n",
    "6. Plotly and Dash customization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:05:56.947516Z",
     "start_time": "2020-05-22T22:05:56.874389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aaccfe9bc845a4a406c4ed89d7e6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(box_style='warning', children=(VBox(children=(Label(value='iHelp Menu: View Help and/or Source Code'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d621e1a33b64a0b82796b1a3e4ebb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 4. ipywidgets Example\n",
    "try: \n",
    "    import fsds_100719 as fs\n",
    "except:\n",
    "    !pip install -U fsds_100719 \n",
    "    import fsds_100719 as fs\n",
    "\n",
    "fs.ihelp_menu(fs.ihelp_menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO: Using HTML & CSS in MARKDOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text that produced the \"YOUR NOTEBOOKS\" bullet above:\n",
    "\n",
    "```html\n",
    "\n",
    "5. <div style=\"color:blue;text-align:center;border: solid purple 2px;font-family:serif;\">YOUR NOTEBOOKS!</div>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding/controlling images and alignment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T21:59:46.204532Z",
     "start_time": "2020-05-22T21:59:46.131445Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Style Sheets/webscrape_example.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2e6473749eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Style Sheets/webscrape_example.css\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mas\u001b[0m \u001b[0mf_css\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_css\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<style>\"\u001b[0m\u001b[0;34m\"</style>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.format(style))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Style Sheets/webscrape_example.css'"
     ]
    }
   ],
   "source": [
    "with open(\"Style Sheets/webscrape_example.css\",'r')  as f_css:\n",
    "    style = f_css.read()\n",
    "HTML(\"<style>\"\"</style>\")#.format(style))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.784017Z",
     "start_time": "2020-05-22T00:30:05.715Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dMtG8DfLtWzG"
   },
   "outputs": [],
   "source": [
    "# fs.jmi.ihelp_menu(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's add an image hosted on github to our notebook (great example for making blog posts).\n",
    "1. Go to the repo's website, click on the image file.\n",
    "2. Click download and copy the raw.githubsercontent.link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add an image hosted on github by grabbing grabbing the raw link.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Images/flatiron-building-glitter.jpeg\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO: Loading CSS Via Python & Using VS Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VS Code has amazingly helpful html and css autocomplete.\n",
    "- We can load external CSS files by using `IPython.display.HTML` and using the code \n",
    "\n",
    "\n",
    "```python\n",
    "from IPython.display import HTML\n",
    "HTML(\"<style>{}</style>\".format(css_info))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.784880Z",
     "start_time": "2020-05-22T00:30:05.720Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "with open(\"Style Sheets/webscrape_example.css\") as css_f:\n",
    "    css_info = css_f.read()\n",
    "\n",
    "css_info=\"\"\n",
    "HTML(\"<style>{}</style>\".format(css_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTuSwL_QE21F"
   },
   "source": [
    "<div id=\"part2header\"> <h1>ðŸ““ Part 2: Basics of Web Scraping</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.785744Z",
     "start_time": "2020-05-22T00:30:05.723Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gh6WTxiVYqkh"
   },
   "outputs": [],
   "source": [
    "!pip install fsds_100719, fake_useragent,lxml\n",
    "# !pip install fake_useragent\n",
    "# !pip install lxml\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8RaN-P7-aO7"
   },
   "source": [
    "\n",
    "\n",
    "### Recommended packages/tools to use\n",
    "1. `fake_useragent`\n",
    "    - pip-installable module that conveniently supplies fake user agent information to use in your request headers.\n",
    "    - recommended by udemy course\n",
    "2. `lxml`\n",
    "    - popular pip installable html parser (recommended by Udemy course)\n",
    "    - using `'html.parser'` in requests.get() did not work for me, I had to install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaWIHHicwtAr"
   },
   "source": [
    "## Using python's `requests` module:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JpBMzWysPb0"
   },
   "source": [
    "\n",
    "-  Use `requests` library to initiate connections to a website.\n",
    "- Check the status code returned to determine if connection was successful (status code=200)\n",
    "\n",
    "```python\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "\n",
    "# Connect to the url using requests.get\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "```\n",
    "\n",
    " ___\n",
    "| Status Code | Code Meaning \n",
    "| --------- | -------------|\n",
    "1xx |   Informational\n",
    "2xx|    Success \n",
    "3xx|     Redirection\n",
    "4xx|     Client Error \n",
    "5xx |    Server Error\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bJEevoTDebn"
   },
   "source": [
    "\n",
    "- **`response` is a dictionary with the contents printed below**\n",
    "\n",
    "- Adding a sleep time is helpful for avoiding and getting blocked from a server `time.sleep(\n",
    "- **Note: You can add a `timeout` to `requests.get()` to avoid indefinite waiting**\n",
    "    - Best in multiples of 3 (`timeout=3` or `6` , `9` ,etc.)\n",
    "\n",
    "```python\n",
    "# Add a timeout to prevent hanging\n",
    "response = requests.get(url, timeout=3)\n",
    "response.status_code\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.786906Z",
     "start_time": "2020-05-22T00:30:05.727Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "\n",
    "response = requests.get(url=url, timeout=3)\n",
    "print('Status code: ',response.status_code)\n",
    "\n",
    "if response.status_code==200:\n",
    "    print('Connection successfull.\\n\\n')\n",
    "else: \n",
    "    print('Error. Check status code table.\\n\\n')  \n",
    "\n",
    "page_content = response.content    \n",
    "soup = bs4.BeautifulSoup(page_content,'lxml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.787786Z",
     "start_time": "2020-05-22T00:30:05.729Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.788756Z",
     "start_time": "2020-05-22T00:30:05.732Z"
    }
   },
   "outputs": [],
   "source": [
    "find_class=\"vertical-navbox nowraplinks plainlist\"\n",
    "\n",
    "soup.find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.789784Z",
     "start_time": "2020-05-22T00:30:05.734Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AEfuIseSn328"
   },
   "outputs": [],
   "source": [
    "## Lets make this a function:\n",
    "def get_response(url = 'https://en.wikipedia.org/wiki/Stock_market',timeout=3,\n",
    "                 verbose=2):\n",
    "    \"\"\"Getting and previewing the website urls response.\n",
    "\n",
    "    Args: \n",
    "        url (str): page to get\n",
    "        timeout (int):  time to delay request\n",
    "        verbose (0,1,2): controls info display. 1= header, 2=header+status_code\n",
    "\n",
    "    Returns:\n",
    "        response (Response)\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    response = requests.get(url=url, timeout=timeout)\n",
    "    if verbose>1:\n",
    "        print('Status code: ',response.status_code)\n",
    "\n",
    "        if response.status_code==200:\n",
    "            print('Connection successfull.\\n\\n')\n",
    "        else: \n",
    "            print('Error. Check status code table.\\n\\n')    \n",
    "\n",
    "    if verbose >0:    \n",
    "        # Print out the contents of a request's response\n",
    "        print(f\"{'---'*20}\\n\\tContents of Response.items():\\n{'---'*20}\")\n",
    "\n",
    "        for k,v in response.headers.items():\n",
    "            print(f\"{k:{25}}: {v:{40}}\") # Note: add :{number} inside of a   \n",
    "\n",
    "    return response\n",
    "        \n",
    "response = get_response(verbose=0)\n",
    "# help(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.790560Z",
     "start_time": "2020-05-22T00:30:05.736Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "w5Rn-Y0N0q3B"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "\n",
    "# response = requests.get(url, timeout=3)\n",
    "# print('Status code: ',response.status_code)\n",
    "# if response.status_code==200:\n",
    "#     print('Connection successfull.\\n\\n')\n",
    "# else: \n",
    "#     print('Error. Check status code table.\\n\\n')    \n",
    "\n",
    "    \n",
    "# # Print out the contents of a request's response\n",
    "# print(f\"{'---'*20}\\n\\tContents of Response.items():\\n{'---'*20}\")\n",
    "\n",
    "# for k,v in response.headers.items():\n",
    "#     print(f\"{k:{25}}: {v:{40}}\") # Note: add :{number} inside of a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.791589Z",
     "start_time": "2020-05-22T00:30:05.739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fbpLnJrzDetE"
   },
   "outputs": [],
   "source": [
    "# for k,v in response.headers.items():\n",
    "#     print(f\"{k:{30}}:{v:{20}}\") # Note: add :{number} inside of a  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAPbGwxTDZbG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjXVI4Qpw2_n"
   },
   "source": [
    "### Sidebar Notes - Explaining The Above Text Printing/Formatting:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkwN3WP80tiS"
   },
   "source": [
    "\n",
    "- **You can repeat strings by using multiplication**\n",
    "    - `'---'*20` will repeat the dashed lines 20 times\n",
    "\n",
    "- **You can determine how much space is alloted for a variable when using f-strings**\n",
    "    - Add a `:{##}` after the variable to specify the allocated width\n",
    "    - Add a `>` before the `{##}` to force alignment \n",
    "    - Add another symbol (like '.'' or '-') before `>` to add guiding-line/placeholder (like in a table of contents)\n",
    "\n",
    "```python\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(f\"Status code: {response.status_code:>{20}}\")\n",
    "print(f\"Status code: {response.status_code:->{20}}\")\n",
    "```    \n",
    "```\n",
    "# Returns:\n",
    "Status code: 200\n",
    "Status code:                  200\n",
    "Status code: -----------------200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO3ac1hE8gr5"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7H_4da2vkGL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "we-zGs8lw7kY"
   },
   "source": [
    "##  Using `BeautifulSoup`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mPEFZu5pjdR"
   },
   "source": [
    "### Cook a soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDQT-D171lhn"
   },
   "source": [
    "\n",
    "\n",
    "- Connect to a website using`response = requests.get(url)`\n",
    "- Feed `response.content` into BeautifulSoup \n",
    "- Must specify the parser that will analyze the contents\n",
    "    - default available is `'html.parser'`\n",
    "    - recommended is to install and use `lxml` [[lxml documentation](https://lxml.de/3.7/)]\n",
    "- use soup.prettify() to get a user-friendly version of the content to print\n",
    "\n",
    "```python\n",
    "# Define Url and establish connection\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "response = requests.get(url, timeout=3)\n",
    "\n",
    "# Feed the response's .content into BeauitfulSoup\n",
    "page_content = response.content\n",
    "soup = BeautifulSoup(page_content,'lxml') #'html.parser')\n",
    "\n",
    "# Preview soup contents using .prettify()\n",
    "print(soup.prettify()[:2000])\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.792393Z",
     "start_time": "2020-05-22T00:30:05.745Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GGI05UfxoDaP"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "response = get_response(verbose=0)\n",
    "\n",
    "c = response.content\n",
    "# feed content into a beautiful soup using lxml\n",
    "soup = BeautifulSoup(c,'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.793419Z",
     "start_time": "2020-05-22T00:30:05.747Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Lj8ASjVnouHF"
   },
   "outputs": [],
   "source": [
    "## Lets make this into a function\n",
    "def placeholder2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rkqfa-kvE-7Q"
   },
   "source": [
    "## What's in a Soup?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.794242Z",
     "start_time": "2020-05-22T00:30:05.750Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z9QqWYYlophQ"
   },
   "outputs": [],
   "source": [
    "help(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.795292Z",
     "start_time": "2020-05-22T00:30:05.752Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fUBqsrfO0nNO"
   },
   "outputs": [],
   "source": [
    "# soup.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FieLZ63VVEXi"
   },
   "source": [
    "- **A soup is essentially a collection of `tag objects`**\n",
    "    - each tag from the html is a tag object in the soup\n",
    "    - the tag's maintain the hierarchy of the html page, so tag objects will contain _other_ tag objects that were under it in the html tree.\n",
    "\n",
    "- **Each tag has a:**\n",
    "    - `.name`\n",
    "    - `.contents`\n",
    "    - `.string`\n",
    "    \n",
    "- **A tag can be access by name (like a column in a dataframe using dot notation)**\n",
    "    - and then you can access the tags within the new tag-variable just like the first tag\n",
    "    ```python\n",
    "    # Access tags by name\n",
    "    meta = soup.meta\n",
    "    head = soup.head\n",
    "    body = soup.body\n",
    "    # and so on...\n",
    "    ```\n",
    "- [!] ***BUT this will only return the FIRST tag of that type, to access all occurances of a tag-type, we will need to navigate the html family tree***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZhVt8BFCXz"
   },
   "source": [
    "\n",
    "### Navigating the HTML Family Tree: Children, siblings, and parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZCLylw9RkVL"
   },
   "source": [
    "\n",
    "\n",
    "- **Each tag is located within a tree-hierarchy of parents, siblings, and children**\n",
    "    - The family-relation is based on the identation level of the tags.\n",
    "\n",
    "- **Methods/attributes for the location/related tags of a tag**\n",
    "    - `.parent`, `.parents`\n",
    "    - `.child`, `.children`\n",
    "    - `.descendents`\n",
    "    - `.next_sibling`, `.previous_sibling`\n",
    "\n",
    "- *Note: a newline character `\\n` is also considered a tag/sibling/child*\n",
    "\n",
    "#### Accessing Child Tags\n",
    "\n",
    "- To get to later occurances of a tag type (i.e. the 2nd `<p>` tag in a tree), we need to navigate through the parent tag's `children`\n",
    "    - To access an iterable list of a tag's children use `.children`\n",
    "        - But, this only returns its *direct children*  (one indentation level down)     \n",
    "        \n",
    "    ```python\n",
    "    # print direct children of the body tag\n",
    "    body = soup.body\n",
    "    for child in body.children:\n",
    "        # print child if its not empty\n",
    "        print(child if child is not None else ' ', '\\n\\n')  # '\\n\\n' for visual separation\n",
    "    ```\n",
    "- To access *all children* use `.descendents`\n",
    "    - Returns all chidren and children of children\n",
    "    ```python\n",
    "    for child in body.descendents:\n",
    "        # print all children/grandchildren, etc\n",
    "        print(child if child is not None else ' ','\\n\\n')  \n",
    "    ```\n",
    "    \n",
    "#### Accessing Parent tags\n",
    "\n",
    "- To access the parent of a tag use `.parent`\n",
    "```python\n",
    "title = soup.head.title\n",
    "print(title.parent.name)\n",
    "```\n",
    "\n",
    "- To get a list of _all parents_ use `.parents`\n",
    "```python\n",
    "title = soup.head.title\n",
    "for parent in title.parents:\n",
    "    print(parent.name)\n",
    "```\n",
    "\n",
    "#### Accessing Sibling tags\n",
    "- siblings are tags in the same tree indentation level\n",
    "- `.next_sibling`, `.previous_sibling`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTDsYxx6Gafk"
   },
   "source": [
    "## Searching Through Soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vfo7esGvpMm9"
   },
   "source": [
    "### Finding the target tags to isolate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXktQsDvWW0A"
   },
   "source": [
    "\n",
    "\n",
    "Using example  from  [Wikipedia article](https://en.wikipedia.org/wiki/Stock_market)\n",
    "where we are trying to isolate the body of the article content.\n",
    "\n",
    "\n",
    "- **Examine the website using Chrome's inspect view.**\n",
    "\n",
    "    - Press F12 or right-click > inspect\n",
    "\n",
    "    - Use the mouse selector tool (top left button) to explore the web page content for your desired target\n",
    "        - the web page element will be highlighted on the page itself and its corresponding entry in the document tree.\n",
    "        - Note: click on the web page with the selector in order to keep it selected in the document tree\n",
    "\n",
    "    - Take note of any identifying attributes for the target tag (class, id, etc)\n",
    "<img src=\"https://drive.google.com/uc?export-download&id=1KifQ_ukuXFdnCh1Tz1rwzA_cWkB_45mf\" width=450>\n",
    "\n",
    "### Using BeautifulSoup's search functions\n",
    "Note: while the process below is a decent summary, there is more nuance to html/css tags than I personally have been able to digest. \n",
    "    - If something doesn't work as expected/explained, please verify in the documentation.\n",
    "        - [BeauitfulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation)\n",
    "        - [docs for .find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)\n",
    "    \n",
    "- **BeautifulSoup has methods for searching through descendent-tags**\n",
    "    - `.find`\n",
    "    - `.find_all`\n",
    "    \n",
    "- **Using `.find_all()`**\n",
    "    - Searches through all descendent tags and returns a result set (list of tag objects)\n",
    "```python\n",
    "# How to get results from .find_all()\n",
    "results = soup.find_all(name, attrs, recursive, string, limit,**kwargs) `\n",
    "```        \n",
    "    - `.find_all()` parameters:\n",
    "        - `name` _(type of tags to consider)_\n",
    "            - only consider tags with this name \n",
    "                - Ex: 'a',  'div', 'p' ,etc.\n",
    "        - `atrrs`_(css attributes that you are looking for in your target tag)_\n",
    "            - enter an attribute such as the class or id as a string\n",
    "\n",
    "                `attrs='mw-content-ltr'`\n",
    "            - if passing more than one attribute, must use a dictionary:\n",
    "\n",
    "            `attrs={'class':'mw-content-ltr', 'id':'mw-content-text'}`\n",
    "        - `recursive`_(Default=True)_\n",
    "            - search all children (`True`)\n",
    "            - search only  direct children(`False`)\n",
    "\n",
    "        - `string`\n",
    "            - search for text _inside_ of tags instead of the tags themselves\n",
    "            - can be regular expression\n",
    "        - `limit`\n",
    "            - How many results you want it to return\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.796661Z",
     "start_time": "2020-05-22T00:30:05.757Z"
    }
   },
   "outputs": [],
   "source": [
    "fs.ihelp(get_response,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.797857Z",
     "start_time": "2020-05-22T00:30:05.760Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "MAjyd1X72tKK",
    "outputId": "1205ff75-87c5-40c7-a957-b3c67d4c3471"
   },
   "outputs": [],
   "source": [
    "response = get_response(url='https://www.ebay.com/')\n",
    "\n",
    "c = response.content\n",
    "# feed content into a beautiful soup using lxml\n",
    "soup = BeautifulSoup(c,'lxml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.798657Z",
     "start_time": "2020-05-22T00:30:05.763Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Gf8pI5VK3Fwx"
   },
   "outputs": [],
   "source": [
    "target_class= 'hl-item__displayPrice secondary-text'\n",
    "tags = soup.find_all('span',attrs={'class':target_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.799806Z",
     "start_time": "2020-05-22T00:30:05.765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "f2EsZDhn3Ftx",
    "outputId": "c89d2b4d-471b-4fef-c288-53115575cecb"
   },
   "outputs": [],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.800600Z",
     "start_time": "2020-05-22T00:30:05.767Z"
    }
   },
   "outputs": [],
   "source": [
    "tag0 = tags[0]\n",
    "[print(tag) for tag in tags[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.801590Z",
     "start_time": "2020-05-22T00:30:05.770Z"
    }
   },
   "outputs": [],
   "source": [
    "all_text = \"\"\"<span class=\"hl-item__displayPrice secondary-text\"><!--M^s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[0]-8[0]-0-15[0[0]] s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[0]-8[0] 15[0[0]]-->$89.99<!--M/--></span>\n",
    "<span class=\"hl-item__displayPrice secondary-text\"><!--M^s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[1]-8[1]-0-15[0[0]] s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[1]-8[1] 15[0[0]]-->$59.99<!--M/--></span>\n",
    "<span class=\"hl-item__displayPrice secondary-text\"><!--M^s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[2]-8[2]-0-15[0[0]] s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[2]-8[2] 15[0[0]]-->$85.99<!--M/--></span>\n",
    "<span class=\"hl-item__displayPrice secondary-text\"><!--M^s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[3]-8[3]-0-15[0[0]] s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[3]-8[3] 15[0[0]]-->$85.99<!--M/--></span>\n",
    "<span class=\"hl-item__displayPrice secondary-text\"><!--M^s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[4]-8[4]-0-15[0[0]] s0-0-3_1-0-2[2]-4-match-media-0-ebay-carousel-4[4]-8[4] 15[0[0]]-->$115.48<!--M/--></span>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.802440Z",
     "start_time": "2020-05-22T00:30:05.772Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regexp = re.compile(r\"(\\$\\d{1,})\\.(\\d{2})\")\n",
    "regexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.803078Z",
     "start_time": "2020-05-22T00:30:05.775Z"
    }
   },
   "outputs": [],
   "source": [
    "found_text = regexp.findall(all_text)\n",
    "found_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.804005Z",
     "start_time": "2020-05-22T00:30:05.778Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NJkRhE6f3Frp",
    "outputId": "88b55477-5142-49c9-8e62-f3b727d697ee"
   },
   "outputs": [],
   "source": [
    "tag0=tags[0]\n",
    "target = tag0.contents\n",
    "target = ' '.join(target)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidebar: Using RegularExpressions to sift through the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mnqtIzLFmXR"
   },
   "source": [
    "\n",
    "- Best Hands On Tester for Regex:\n",
    "    - https://regex101.com/\n",
    "    - Select \"Python\" on the left side of the page.\n",
    "    - Paste the text you want to sift through in the large center window.\n",
    "    - Type your expression in the top center window.\n",
    "    - It will highlight the text that matches your regular expression in the big center panel. \n",
    "\n",
    "- Cheatsheet for Regex Symbols:\n",
    "    - https://www.debuggex.com/cheatsheet/regex/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.804898Z",
     "start_time": "2020-05-22T00:30:05.781Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ARB9GOg-3Fps",
    "outputId": "47933797-b731-4012-b612-959341806d10"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "price =  re.compile(\"(\\$\\d\\,\\d*\\.\\d{2})\")\n",
    "price.findall(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ðŸ““ Part 3: Web Scraping Lab\n",
    "- [Web Scraping Learn Lesson](https://learn.co/tracks/data-science-career-v2/module-2-data-engineering-for-data-science/section-13-html-css-and-web-scraping/web-scraping-in-practice)\n",
    "\n",
    "- [Learn Lab](https://learn.co/tracks/data-science-career-v2/module-2-data-engineering-for-data-science/section-13-html-css-and-web-scraping/web-scraping-lab)\n",
    "    - [GitHub solution](https://github.com/jirvingphd/dsc-web-scraping-lab-online-ds-ft-100719/tree/solution)\n",
    "    \n",
    "- Jump to other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "NIIRHp-T5PvA"
   },
   "source": [
    "___\n",
    "# END OF CLASS MATERIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr65hBzg5Phe"
   },
   "source": [
    "# BONUS FUNCTIONS: didn't get to in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.909784Z",
     "start_time": "2020-05-22T00:30:05.906225Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4RM3tQkqyuoe"
   },
   "outputs": [],
   "source": [
    "def get_all_links(soup):#,attr_kwds=None):\n",
    "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds),which will be used in soup.findAll(attrs=attr_kwds).\n",
    "    Returns a list of links.\n",
    "    tag_type = 'a' or 'href'\"\"\"\n",
    "    all_a_tags = soup.findAll('a',attrs=kwds) \n",
    "    link_list = []\n",
    "    for link in all_a_tags:\n",
    "        test_link = link.get('href')#,attr=kwds)\n",
    "#         test_link = link.get('href',attrs=kwds)\n",
    "        link_list.append(test_link)\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.914398Z",
     "start_time": "2020-05-22T00:30:05.911099Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UNjG2XYFyPIH"
   },
   "outputs": [],
   "source": [
    "def make_absolute_links(source_url, rel_link_list):\n",
    "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    absolute_links=[]\n",
    "\n",
    "    # Create a for loop to loop through links and make absolute html paths\n",
    "    for link in rel_link_list:\n",
    "\n",
    "        # Get base url using a url pasers and the story_url at the beginning of the nb\n",
    "        abs_link = urljoin(source_url,link)    \n",
    "\n",
    "        #concatenate and append to a list \n",
    "        absolute_links.append(abs_link)\n",
    "    \n",
    "    return absolute_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVfjE8RhHZq-"
   },
   "source": [
    "## Ex Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xBlb5NuyeAC"
   },
   "source": [
    "- `soup = cook_soup_from_url(url)`\n",
    "    - make a beautiful soup from url\n",
    "-`soup_links = get_all_links(soup)`\n",
    "    - get all links from soup and return as a list.\n",
    "    \n",
    "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
    "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
    "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.919523Z",
     "start_time": "2020-05-22T00:30:05.915841Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UwR64_fYv3Up"
   },
   "outputs": [],
   "source": [
    "def cook_soup_from_url(url, parser='lxml',sleep_time=0):\n",
    "    \"\"\"Uses requests to retreive webpage and returns a BeautifulSoup made using lxml parser.\"\"\"\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    sleep(sleep_time)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # check status of request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Error: Status_code !=200.\\n status_code={response.status_code}')\n",
    "                        \n",
    "    c = response.content\n",
    "    # feed content into a beautiful soup using lxml\n",
    "    soup = BeautifulSoup(c,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.928774Z",
     "start_time": "2020-05-22T00:30:05.920904Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Zo1sJyNE6ZCb"
   },
   "outputs": [],
   "source": [
    "def cook_batch_of_soups(link_list, sleep_time=1): #,user_fun = extract_target_text):\n",
    "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
    "    with their relative url path as their key.\n",
    "    Set user_fun to None to just extract full soups without user_extract\"\"\"\n",
    "    from time import sleep\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    batch_of_soups = []\n",
    "    \n",
    "    for link in link_list:\n",
    "        soup_dict = {}\n",
    "        \n",
    "        \n",
    "        # turn the url path into the dictionary key/title\n",
    "        url_dict_key_path = urlparse(link).path\n",
    "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
    "        \n",
    "        soup_dict['_url'] = link\n",
    "        soup_dict['path'] = url_dict_key\n",
    "\n",
    "        # make a soup from the current link\n",
    "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
    "        soup_dict['soup'] = page_soup\n",
    "\n",
    "        \n",
    "#         if user_fun!=None:\n",
    "#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION       \n",
    "#             user_output = user_fun(page_soup) #can add inputs to function\n",
    "#             soup_dict['user_extract'] = user_output\n",
    "        \n",
    "        # Add current page's soup to batch_of_soups list\n",
    "        batch_of_soups.append(soup_dict)\n",
    "        \n",
    "    return batch_of_soups\n",
    "\n",
    "\n",
    "def extract_target_text(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "    if attrs_dict==None:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "    else:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "    # if extracting from multiple tags\n",
    "    output=[]\n",
    "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "    if join_text == True:\n",
    "        output = ' '.join(output)\n",
    "\n",
    "    ## ADDING SAVING EACH \n",
    "    if save_files==True:\n",
    "        text = output #soup.body.string\n",
    "        filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "        soup_dict['filename'] = filename\n",
    "        with open(filename,'w+') as f:\n",
    "            f.write(text)\n",
    "        print(f'File  successfully saved as {filename}')\n",
    "\n",
    "    return  output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:05.934050Z",
     "start_time": "2020-05-22T00:30:05.930007Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0ODwEjKH92T5"
   },
   "outputs": [],
   "source": [
    "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
    "    import pickle\n",
    "    import sys\n",
    "    \n",
    "    filepath = save_location+pickle_name\n",
    "    \n",
    "    with open(filepath,'wb') as f:\n",
    "        pickle.dump(soups, f)\n",
    "        \n",
    "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
    "\n",
    "def load_leftovers(filepath):\n",
    "    import pickle\n",
    "    \n",
    "    print(f'Opening leftovers: {filepath}')\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        leftover_soup = pickle.load(f)\n",
    "        \n",
    "    return leftover_soup\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ng-HN_rRymjI"
   },
   "source": [
    "#### Walkthrough - using James' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:08.187727Z",
     "start_time": "2020-05-22T00:30:05.935293Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1Pg3VWWmypij"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "soup = cook_soup_from_url(url,sleep_time=1)\n",
    "\n",
    "\n",
    "## Get all links that match are interal wikipedia redirects [yes?]\n",
    "kwds = {'class':'mw-redirect'}\n",
    "links = get_all_links(soup)#,kwds)\n",
    "\n",
    "\n",
    "# preview first 5 links\n",
    "print(links[:5])\n",
    "\n",
    "# Turn relative links into absolute links\n",
    "abs_links = make_absolute_links(url,links)\n",
    "print(abs_links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:19.425180Z",
     "start_time": "2020-05-22T00:30:08.189379Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6tCwXLiN7UCk"
   },
   "outputs": [],
   "source": [
    "# Selecting only the first 5 links to test\n",
    "abs_links_for_soups = abs_links[:5]\n",
    "\n",
    "\n",
    "# Cooking a batch of soups from those chosen links\n",
    "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
    "\n",
    "# batch_of_soups is a list as long as the input link_list\n",
    "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
    "\n",
    "# batch_of_soups is a list of soup-dictionaries\n",
    "soup_dict = batch_of_soups[0]\n",
    "print('Each soup_dict has ',soup_dict.keys())\n",
    "\n",
    "# the page's soup is stored under soup_dict['soup']\n",
    "soup_from_soup_dict = soup_dict['soup']\n",
    "type(soup_from_soup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFVnj20YmQK2"
   },
   "source": [
    "#### Notes on extracting content.\n",
    "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:19.447901Z",
     "start_time": "2020-05-22T00:30:19.426389Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QwU8aPHJhAVm"
   },
   "outputs": [],
   "source": [
    "## ADDING extract_target_text to precisely target text\n",
    "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "#     if attrs_dict==None:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "#     else:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "#     # if extracting from multiple tags\n",
    "#     output=[]\n",
    "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "#     if join_text == True:\n",
    "#         output = ' '.join(output)\n",
    "\n",
    "#     ## ADDING SAVING EACH \n",
    "#     if save_files==True:\n",
    "#         text = output #soup.body.string\n",
    "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "#         soup_dict['filename'] = filename\n",
    "#         with open(filename,'w+') as f:\n",
    "#             f.write(text)\n",
    "#         print(f'File  successfully saved as {filename}')\n",
    "\n",
    "#     return  output\n",
    "\n",
    "# ####################\n",
    "\n",
    "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
    "for i, soup_dict in enumerate(batch_of_soups):\n",
    "    \n",
    "    # Get the soup from the dict\n",
    "    soup = soup_dict['soup']\n",
    "    \n",
    "    # Extract text \n",
    "    extracted_text = extract_target_text(soup)\n",
    "    \n",
    "    # Add key:value for results of extract\n",
    "    soup_dict['extracted'] = extracted_text\n",
    "    \n",
    "    # Replace the old soup_dict with the new one with 'extracted'\n",
    "    batch_of_soups[i] = soup_dict\n",
    "    \n",
    "example_extracted_text=batch_of_soups[0]['extracted']\n",
    "print(example_extracted_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:30:19.451789Z",
     "start_time": "2020-05-22T00:30:19.449640Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eUJ50zdSvjyL"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from fake_useragent import UserAgent\n",
    "# ua = UserAgent()\n",
    "\n",
    "# header = {'user-agent':ua.chrome}\n",
    "# print('Header:\\n',header)\n",
    "\n",
    "# url ='https://en.wikipedia.org/wiki/Stock_market'\n",
    "# response = requests.get(url, timeout=3, headers=header)\n",
    "\n",
    "# print('Status code: ',response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFlH7PNCFX58"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KjXVI4Qpw2_n",
    "eVfjE8RhHZq-"
   ],
   "name": "Web Scraping 101.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
